#!/usr/bin/env python3
"""
LINE å°è©±è¨˜éŒ„åˆ†æè…³æœ¬
å¾ line_history_data è³‡æ–™å¤¾ä¸­æŠ½æ¨£ä¸¦åˆ†æå°è©±è¨˜éŒ„
"""

import os
import csv
import json
import random
from collections import defaultdict, Counter
from pathlib import Path
from datetime import datetime

class LineConversationAnalyzer:
    def __init__(self, data_dir, sample_size=100):
        self.data_dir = Path(data_dir)
        self.sample_size = sample_size
        self.conversations = []
        self.qna_pairs = []
        self.stats = defaultdict(int)
        self.categories = defaultdict(list)
        
    def find_valid_files(self):
        """æ‰¾å‡ºæ‰€æœ‰æœ‰æ•ˆçš„ CSV æª”æ¡ˆ (>1KB)"""
        valid_files = []
        for csv_file in self.data_dir.glob("*.csv"):
            if csv_file.stat().st_size > 1024:  # >1KB
                valid_files.append(csv_file)
        return valid_files
    
    def sample_files(self, valid_files):
        """éš¨æ©ŸæŠ½æ¨£æŒ‡å®šæ•¸é‡çš„æª”æ¡ˆ"""
        if len(valid_files) <= self.sample_size:
            return valid_files
        return random.sample(valid_files, self.sample_size)
    
    def parse_csv(self, csv_file):
        """è§£æå–®ä¸€ CSV æª”æ¡ˆ"""
        conversation = {
            'file': csv_file.name,
            'messages': [],
            'user_messages': [],
            'bot_messages': []
        }
        
        try:
            with open(csv_file, 'r', encoding='utf-8') as f:
                # è·³éå‰ 4 è¡Œæ¨™é¡Œ
                for _ in range(4):
                    next(f)
                
                reader = csv.reader(f)
                for row in reader:
                    if len(row) >= 5:
                        sender_type, sender_name, date, time, content = row[:5]
                        
                        message = {
                            'type': sender_type,
                            'sender': sender_name,
                            'datetime': f"{date} {time}",
                            'content': content.strip()
                        }
                        
                        conversation['messages'].append(message)
                        
                        if sender_type == 'User':
                            conversation['user_messages'].append(content.strip())
                        elif sender_type == 'Account':
                            conversation['bot_messages'].append(content.strip())
        
        except Exception as e:
            print(f"Error parsing {csv_file.name}: {e}")
            return None
        
        return conversation if conversation['messages'] else None
    
    def extract_qna_pairs(self, conversation):
        """å¾å°è©±ä¸­æå–å•ç­”å°"""
        pairs = []
        messages = conversation['messages']
        
        for i in range(len(messages) - 1):
            if messages[i]['type'] == 'User' and messages[i+1]['type'] == 'Account':
                question = messages[i]['content']
                answer = messages[i+1]['content']
                
                # éæ¿¾ç„¡æ„ç¾©çš„å°è©±
                if self.is_valid_qna(question, answer):
                    pairs.append({
                        'question': question,
                        'answer': answer,
                        'source': conversation['file']
                    })
        
        return pairs
    
    def is_valid_qna(self, question, answer):
        """åˆ¤æ–·å•ç­”å°æ˜¯å¦æœ‰æ•ˆ"""
        # éæ¿¾æ¢ä»¶
        invalid_keywords = ['ç…§ç‰‡å·²å‚³é€', 'è²¼åœ–å·²å‚³é€', 'Unknown', 'ç³»çµ±å¿™ç¢Œ', 'é€£ç·šæœ‰é»å•é¡Œ']
        
        if len(question) < 2 or len(answer) < 10:
            return False
        
        for keyword in invalid_keywords:
            if keyword in question or keyword in answer:
                return False
        
        return True
    
    def categorize_question(self, question):
        """åˆ†é¡å•é¡Œé¡å‹"""
        categories = {
            'è¨‚æˆ¿æŸ¥è©¢': ['è¨‚å–®', 'ç·¨è™Ÿ', 'é è¨‚', 'è¨‚æˆ¿', 'æˆåŠŸ'],
            'å¤©æ°£æŸ¥è©¢': ['å¤©æ°£', 'æ°£æº«', 'ä¸‹é›¨', 'æ™´å¤©'],
            'è¨­æ–½æœå‹™': ['åœè»Š', 'æ—©é¤', 'wifi', 'ç¶²è·¯', 'check-in', 'check-out', 'é€€æˆ¿', 'å…¥ä½'],
            'ä½ç½®äº¤é€š': ['åœ°å€', 'æ€éº¼å»', 'è»Šç«™', 'äº¤é€š', 'å°è¦½'],
            'æˆ¿å‹åƒ¹æ ¼': ['æˆ¿å‹', 'æˆ¿é–“', 'åƒ¹æ ¼', 'å¤šå°‘éŒ¢', 'è²»ç”¨'],
            'ä¸€èˆ¬å•å€™': ['ä½ å¥½', 'æ‚¨å¥½', 'å—¨', 'hi', 'hello'],
        }
        
        q_lower = question.lower()
        for category, keywords in categories.items():
            for keyword in keywords:
                if keyword in q_lower:
                    return category
        
        return 'å…¶ä»–'
    
    def analyze(self):
        """åŸ·è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸ” é–‹å§‹åˆ†æ LINE å°è©±è¨˜éŒ„...")
        
        # 1. æ‰¾å‡ºæœ‰æ•ˆæª”æ¡ˆ
        print("ğŸ“ æœå°‹æœ‰æ•ˆæª”æ¡ˆ...")
        valid_files = self.find_valid_files()
        print(f"   æ‰¾åˆ° {len(valid_files)} å€‹æœ‰æ•ˆæª”æ¡ˆ")
        
        # 2. éš¨æ©ŸæŠ½æ¨£
        print(f"ğŸ² éš¨æ©ŸæŠ½æ¨£ {self.sample_size} å€‹æª”æ¡ˆ...")
        sampled_files = self.sample_files(valid_files)
        print(f"   æŠ½æ¨£å®Œæˆï¼š{len(sampled_files)} å€‹æª”æ¡ˆ")
        
        # 3. è§£æå°è©±
        print("ğŸ“Š è§£æå°è©±å…§å®¹...")
        for i, csv_file in enumerate(sampled_files, 1):
            if i % 20 == 0:
                print(f"   é€²åº¦ï¼š{i}/{len(sampled_files)}")
            
            conv = self.parse_csv(csv_file)
            if conv:
                self.conversations.append(conv)
                self.stats['total_messages'] += len(conv['messages'])
                self.stats['total_user_messages'] += len(conv['user_messages'])
                self.stats['total_bot_messages'] += len(conv['bot_messages'])
                
                # æå–å•ç­”å°
                pairs = self.extract_qna_pairs(conv)
                self.qna_pairs.extend(pairs)
                
                # åˆ†é¡çµ±è¨ˆ
                for msg in conv['user_messages']:
                    category = self.categorize_question(msg)
                    self.categories[category].append(msg)
        
        self.stats['analyzed_files'] = len(self.conversations)
        self.stats['valid_qna_pairs'] = len(self.qna_pairs)
        
        print(f"âœ… åˆ†æå®Œæˆï¼")
        print(f"   - åˆ†ææª”æ¡ˆï¼š{self.stats['analyzed_files']}")
        print(f"   - ç¸½è¨Šæ¯æ•¸ï¼š{self.stats['total_messages']}")
        print(f"   - æå–å•ç­”å°ï¼š{self.stats['valid_qna_pairs']}")
    
    def generate_report(self, output_file):
        """ç”¢ç”Ÿ Markdown åˆ†æå ±å‘Š"""
        report = []
        report.append("# LINE å°è©±è¨˜éŒ„åˆ†æå ±å‘Š\n")
        report.append(f"**åˆ†ææ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        report.append(f"**è³‡æ–™ä¾†æº**: line_history_data è³‡æ–™å¤¾\n")
        report.append(f"**åˆ†æç¯„åœ**: {self.stats['analyzed_files']} å€‹å°è©±æª”æ¡ˆ\n\n")
        
        # çµ±è¨ˆæ‘˜è¦
        report.append("## ğŸ“Š çµ±è¨ˆæ‘˜è¦\n\n")
        report.append(f"- **ç¸½è¨Šæ¯æ•¸**: {self.stats['total_messages']:,}\n")
        report.append(f"- **å®¢äººè¨Šæ¯**: {self.stats['total_user_messages']:,}\n")
        report.append(f"- **Bot å›æ‡‰**: {self.stats['total_bot_messages']:,}\n")
        report.append(f"- **æœ‰æ•ˆå•ç­”å°**: {self.stats['valid_qna_pairs']:,}\n")
        report.append(f"- **å¹³å‡æ¯å°è©±è¨Šæ¯æ•¸**: {self.stats['total_messages'] / max(self.stats['analyzed_files'], 1):.1f}\n\n")
        
        # å•é¡Œåˆ†é¡çµ±è¨ˆ
        report.append("## ğŸ“‹ å•é¡Œåˆ†é¡çµ±è¨ˆ\n\n")
        report.append("| é¡åˆ¥ | å•é¡Œæ•¸é‡ | ä½”æ¯” |\n")
        report.append("|------|----------|------|\n")
        
        total_user_msgs = self.stats['total_user_messages']
        for category in sorted(self.categories.keys(), key=lambda x: len(self.categories[x]), reverse=True):
            count = len(self.categories[category])
            percentage = (count / total_user_msgs * 100) if total_user_msgs > 0 else 0
            report.append(f"| {category} | {count} | {percentage:.1f}% |\n")
        
        # å¸¸è¦‹å•é¡Œ TOP 30
        report.append("\n## ğŸ”¥ å¸¸è¦‹å•é¡Œ TOP 30\n\n")
        user_questions = [msg for conv in self.conversations for msg in conv['user_messages']]
        question_counter = Counter(user_questions)
        
        for i, (question, count) in enumerate(question_counter.most_common(30), 1):
            if len(question) > 100:
                question = question[:97] + "..."
            report.append(f"{i}. **{question}** ({count} æ¬¡)\n")
        
        # é«˜å“è³ªå•ç­”å°ç¯„ä¾‹
        report.append("\n## ğŸ’ é«˜å“è³ªå•ç­”å°ç¯„ä¾‹ (å‰ 20 çµ„)\n\n")
        for i, pair in enumerate(self.qna_pairs[:20], 1):
            report.append(f"### {i}. Q: {pair['question']}\n\n")
            answer = pair['answer']
            if len(answer) > 300:
                answer = answer[:297] + "..."
            report.append(f"**A**: {answer}\n\n")
            report.append(f"*ä¾†æº: {pair['source']}*\n\n")
            report.append("---\n\n")
        
        # æ”¹é€²å»ºè­°
        report.append("## ğŸ’¡ æ”¹é€²å»ºè­°\n\n")
        report.append("### 1. çŸ¥è­˜åº«æ“´å……\n")
        report.append(f"- å·²æå– {min(len(self.qna_pairs), 100)} çµ„é«˜å“è³ªå•ç­”å°\n")
        report.append("- å»ºè­°å°‡é€™äº›å•ç­”å°åŠ å…¥ knowledge_base.json\n\n")
        
        report.append("### 2. å¸¸è¦‹å•é¡Œå„ªåŒ–\n")
        for category in ['è¨‚æˆ¿æŸ¥è©¢', 'å¤©æ°£æŸ¥è©¢', 'è¨­æ–½æœå‹™']:
            if category in self.categories:
                report.append(f"- **{category}**: {len(self.categories[category])} æ¬¡æå•ï¼Œå»ºè­°å„ªåŒ–æ­¤é¡å›æ‡‰\n")
        
        # å¯«å…¥æª”æ¡ˆ
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(''.join(report))
        
        print(f"ğŸ“ å ±å‘Šå·²ç”¢ç”Ÿï¼š{output_file}")
    
    def update_knowledge_base(self, kb_file, output_file):
        """æ›´æ–°çŸ¥è­˜åº«"""
        # è®€å–ç¾æœ‰çŸ¥è­˜åº«
        try:
            with open(kb_file, 'r', encoding='utf-8') as f:
                kb = json.load(f)
        except:
            kb = {}
        
        # æå–å‰ 100 çµ„é«˜å“è³ªå•ç­”å°
        new_entries = {}
        for i, pair in enumerate(self.qna_pairs[:100], 1):
            key = f"line_history_{i}"
            new_entries[key] = {
                "question": pair['question'],
                "answer": pair['answer'],
                "source": "LINEå°è©±è¨˜éŒ„åˆ†æ",
                "date_added": datetime.now().strftime('%Y-%m-%d')
            }
        
        # åˆä½µï¼ˆä¸è¦†è“‹ç¾æœ‰è³‡æ–™ï¼‰
        kb.update(new_entries)
        
        # å¯«å…¥æ–°æª”æ¡ˆ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(kb, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“š çŸ¥è­˜åº«å·²æ›´æ–°ï¼š{output_file}")
        print(f"   æ–°å¢ {len(new_entries)} çµ„å•ç­”å°")


if __name__ == "__main__":
    # è¨­å®šåƒæ•¸
    DATA_DIR = "line_history_data"
    SAMPLE_SIZE = 100
    OUTPUT_REPORT = "/Users/like/.gemini/antigravity/brain/e6f49f5c-d2d4-42f9-b533-a65c3916b997/line_chat_analysis.md"
    KB_FILE = "knowledge_base.json"
    KB_OUTPUT = "knowledge_base_updated.json"
    
    # åŸ·è¡Œåˆ†æ
    analyzer = LineConversationAnalyzer(DATA_DIR, SAMPLE_SIZE)
    analyzer.analyze()
    
    # ç”¢ç”Ÿå ±å‘Š
    analyzer.generate_report(OUTPUT_REPORT)
    
    # æ›´æ–°çŸ¥è­˜åº«ï¼ˆç”¢ç”Ÿæ–°æª”æ¡ˆï¼Œä¸ç›´æ¥è¦†è“‹ï¼‰
    analyzer.update_knowledge_base(KB_FILE, KB_OUTPUT)
    
    print("\nâœ… å…¨éƒ¨å®Œæˆï¼")
